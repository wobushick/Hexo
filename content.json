{"meta":{"title":"Haoren's Blog","subtitle":"静心思考，沉浸学习","description":"技术分享 | 学习笔记 | 个人成长","author":"haoren","url":"https://github.io/wobushick/Hexo","root":"/Hexo/"},"pages":[{"title":"","date":"2025-07-15T07:40:10.712Z","updated":"2025-07-15T07:40:10.712Z","comments":true,"path":"about/index.html","permalink":"https://github.io/wobushick/Hexo/about/index.html","excerpt":"","text":"这是我的个人知识库和技术博客，主要用于整理和分享以下内容： 🔍 技术笔记记录环境搭建和工具使用中的技术细节 ✍️ 学习总结整理学习过程中的知识框架和实践经验 🌐 资源索引分类收藏高质量的技术文档和实用资源 🎯 网站目的通过系统化梳理强化知识沉淀保持持续学习与输出的节奏"},{"title":"所有分类","date":"2025-07-15T07:40:10.713Z","updated":"2025-07-15T07:40:10.713Z","comments":true,"path":"categories/index.html","permalink":"https://github.io/wobushick/Hexo/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2025-07-15T07:40:10.713Z","updated":"2025-07-15T07:40:10.713Z","comments":true,"path":"tags/index.html","permalink":"https://github.io/wobushick/Hexo/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"DML数据操作","slug":"DML数据操作","date":"2025-07-14T16:00:00.000Z","updated":"2025-07-15T07:40:10.712Z","comments":true,"path":"2025/07/14/DML数据操作/","permalink":"https://github.io/wobushick/Hexo/2025/07/14/DML%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C/","excerpt":"","text":"数据操作即数据的增删改 Load 将文件导入到hive表中,并不需要查询 ​​LOAD DATA LOCAL INPATH &#39;/local/file/system/path&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (part1=val1, part2=val2 ...)]​​ LOCAL: 表示源文件在客户端本地文件系统。 ​OVERWRITE**:​​ 覆盖目标表&#x2F;分区中的数据。 ​​INTO:​​ 追加数据。 文件会被移动到 Hive 的数据仓库目录中。 LOAD DATA INPATH &#39;hdfs://namenode/path/to/hdfs/file&#39; [OVERWRITE] INTO TABLE ..​​ 源文件在 HDFS 上（无 LOCAL 关键字）。 文件会被移动（而不是复制）到目标 Hive 位置。源文件将不存在于原来路径。 ​​注意:​​ 文件内容必须与表的定义（字段分隔符、集合分隔符等）相匹配。 不会对数据进行任何转换（如 CSV 转 ORC），仅做移动或复制操作。 加载数据后建议运行 ANALYZE TABLE ... COMPUTE STATISTICS 或 msck repair table ... (修复分区元数据) 以优化查询计划。 SELECT 核心:检索表中的数据 ​​关键特性：​​ ​​过滤：​​ 使用 WHERE 子句。 ​​聚合：​​ 使用 GROUP BY 和聚合函数（如 SUM, AVG, COUNT, MAX, MIN）。 ​​排序：​​ 使用 ORDER BY（全局排序）或 SORT BY（Reducer 内排序）。 ​​限制：​​ 使用 LIMIT。 ​​连接：​​ 支持 INNER JOIN, LEFT OUTER JOIN, RIGHT OUTER JOIN, FULL OUTER JOIN, LEFT SEMI JOIN, CROSS JOIN。 ​​视图：​​ 查询结果可以持久化为视图。 ​​子查询：​​ 支持在 FROM、WHERE 和 HAVING 子句中使用子查询（部分旧版本有限制）。 ​​分区裁剪：​​ 查询指定分区可以显著提高性能（WHERE part_column = &#39;value&#39;）。 ​​**DISTRIBUTE BY** &#x2F; **CLUSTER BY**:​​ 控制数据如何分布在 Reducer 之间进行后续处理（如 SORT BY）。 INSERT​ ​​核心：​​ 将查询结果或数据写入 Hive 表。 ​​主要形式:​​ ​INSERT OVERWRITE TABLE target_table [PARTITION (part_col1=val1, part_col2=val2 ...)] SELECT ... FROM ...​ 覆盖：_完全替换_目标表或指定分区中的数据。 ​INSERT INTO TABLE target_table [PARTITION (part_col1=val1, part_col2=val2 ...)] SELECT ... FROM ...​ 追加：将查询结果_追加_到目标表或指定分区中已有数据之后。 ​INSERT OVERWRITE [LOCAL] DIRECTORY &#39;path&#39; [ROW FORMAT ...] [STORED AS ...] SELECT ...​ 将查询结果写入 HDFS 或本地文件系统的目录。 LOCAL 表示写入本地文件系统。 可以指定输出格式和分隔符。 ​​动态分区插入：​​ 在 INSERT 语句中，分区列的值由 SELECT 子句的结果动态确定。 需要设置 hive.exec.dynamic.partition.mode=nonstrict（默认为 strict）。 例如：INSERT OVERWRITE TABLE sales PARTITION (country, state) SELECT ..., se_country, se_state FROM source_table; ​​多表插入 (Multi Table Insert - MTI):​​ 通过一个查询和多个 INSERT 子句，将结果同时写入多个目标表。 语法： 123FROM source_tableINSERT OVERWRITE TABLE target_table1 SELECT col1, col2 ...INSERT INTO TABLE target_table2 SELECT col3, col4 ... UPDATE ​​核心：​​ 修改表中现有行。 ​​关键点：​​ ​​仅适用于事务表：​​ 表必须配置为支持 ACID（原子性、一致性、隔离性、持久性）事务。 ​​前提条件：​​ 表必须是分桶表 (CLUSTERED BY)。 表文件格式必须是 ORC（推荐，性能最佳）。 必须设置 hive.support.concurrency=true, hive.enforce.bucketing=true, hive.exec.dynamic.partition.mode=nonstrict, hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager (或其等效替代品)。 ​​语法：​​ UPDATE tablename SET column = value [, column = value ...] [WHERE expression]; ​​注意：​​ 对于海量数据的更新操作，传统上更高效的方法是使用 INSERT OVERWRITE 重写整个分区或表。 DELETE ​​核心：​​ 删除表中的行。 ​​关键点：​​ ​​仅适用于事务表：​​ 要求与 UPDATE 操作完全相同（分桶表、ORC 格式、ACID 配置）。 ​​语法：​​ DELETE FROM tablename [WHERE expression]; ​​注意：​​ 如果只想删除所有行，TRUNCATE TABLE tablename; 是一个更高效的选项（但它删除整个表数据，不能带 WHERE），但事务表上也要求类似配置。 同样，大规模删除通常更倾向于重写分区。 MERGE(Hivw2.2+引入) ​​核心：​​ 在一条语句中根据条件执行 INSERT, UPDATE, DELETE 操作。也称为 “Upsert”。主要用于变更数据捕获处理。 ​​关键点：​​ ​​仅适用于事务表：​​ 要求与 UPDATE 相同。 ​​语法：​​ 相对复杂，需要指定源表、连接条件以及在匹配和不匹配情况下执行的操作。 基本结构： 1234567MERGE INTO target_table AS TUSING source_table AS SON T.key = S.keyWHEN MATCHED [AND some_condition] THEN UPDATE SET ... [DELETE WHERE ...] -- 可选 DELETE 子句WHEN MATCHED [AND some_condition] THEN DELETE -- 删除匹配行（可选）WHEN NOT MATCHED [AND some_condition] THEN INSERT VALUES (...) -- 插入新行WHEN NOT MATCHED BY SOURCE [AND some_condition] THEN ... -- (可选) 处理目标有但源没有的行 📌 重要注意事项 ​​模式写&#x2F;读 (Schema-on-Read)：​​ Hive 在数据加载时不强制检查数据模式与表定义是否完全匹配。仅在查询时验证。错误的格式在加载过程中可能不会被发现，但在查询时会引发解析错误。 ​​不可变性 (历史限制)：​​ 早期 Hive 设计（非事务表）中 HDFS 文件的不可变性限制了 UPDATE 和 DELETE 的效率。事务表（基于 ORC）通过引入delta文件等机制部分解决了这个问题。 ​​事务表的开销：​​ 虽然事务表支持 ACID，但它们引入了额外的写开销（管理事务、压缩 delta 文件等）。仅当确实需要精细的行级操作时才启用事务功能。 ​​批量操作优先：​​ Hive 最擅长的是批处理操作（大规模的 SELECT 和 INSERT OVERWRITE）。对于频繁的单行 UPDATE/DELETE，传统关系型数据库或 NoSQL 数据库可能更合适。 ​​分区优化：​​ 在大型表上，WHERE 子句包含分区键可以极大地减少需要扫描的数据量（分区裁剪），显著提高查询性能。合理设计分区策略至关重要🔧。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://github.io/wobushick/Hexo/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"DML","slug":"数据库/DML","permalink":"https://github.io/wobushick/Hexo/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/DML/"},{"name":"HQL","slug":"数据库/DML/HQL","permalink":"https://github.io/wobushick/Hexo/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/DML/HQL/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://github.io/wobushick/Hexo/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Hive","slug":"Hive","permalink":"https://github.io/wobushick/Hexo/tags/Hive/"}]},{"title":"DDL数据定义","slug":"DDL数据定义","date":"2025-07-12T16:00:00.000Z","updated":"2025-07-15T07:40:10.712Z","comments":true,"path":"2025/07/12/DDL数据定义/","permalink":"https://github.io/wobushick/Hexo/2025/07/12/DDL%E6%95%B0%E6%8D%AE%E5%AE%9A%E4%B9%89/","excerpt":"","text":"数据库操作创建数据库 123456-- 基础创建CREATE DATABASE IF NOT EXISTS my_db COMMENT &#x27;业务数据仓库&#x27;;-- 指定存储位置CREATE DATABASE my_db LOCATION &#x27;/path/on/hdfs&#x27;; 💡 ​​提示：​​ 使用 IF NOT EXISTS 可避免数据库已存在时报错。 查询数据库123SHOW DATABASES;SHOW DATABASES LIKE &#x27;test*&#x27;; -- 支持模糊匹配DESCRIBE DATABASE EXTENDED my_db; 修改数据库1ALTER DATABASE my_db SET DBPROPERTIES (&#x27;comment&#x27; = &#x27;新描述&#x27;); ⚠️ ​​注意：​​ Hive 只能修改注释和属性，不能重命名或更改位置。 删除数据库12DROP DATABASE IF EXISTS my_db;DROP DATABASE IF EXISTS my_db CASCADE; ⚠️ ​​警告：​​ 生产环境请慎用 CASCADE，防止误删所有表数据。 切换数据库12USE my_db;SHOW TABLES; 表操作创建表基础建表123456789101112CREATE TABLE IF NOT EXISTS employees ( id INT COMMENT &#x27;员工ID&#x27;, name STRING COMMENT &#x27;姓名&#x27;, salary FLOAT COMMENT &#x27;薪资&#x27;, join_date DATE COMMENT &#x27;入职日期&#x27;)COMMENT &#x27;员工信息表&#x27;ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;,&#x27; LINES TERMINATED BY &#x27;\\n&#x27;STORED AS TEXTFILELOCATION &#x27;/user/hive/warehouse/my_db/employees&#x27;; 创建分区表123456CREATE TABLE logs ( log_time TIMESTAMP, message STRING)PARTITIONED BY (dt STRING, region STRING)STORED AS ORC; 复制表结构（不含数据）1CREATE TABLE new_table LIKE old_table; 查看表1234SHOW TABLES;SHOW TABLES LIKE &#x27;emp*&#x27;;DESCRIBE employees;DESCRIBE FORMATTED employees; 修改表重命名表1ALTER TABLE employees RENAME TO staff; 添加列123ALTER TABLE staff ADD COLUMNS ( department STRING COMMENT &#x27;部门&#x27;); 修改列名&#x2F;类型12ALTER TABLE staff CHANGE COLUMN salary salary_info DOUBLE COMMENT &#x27;税前薪资&#x27;; 添加&#x2F;删除&#x2F;修改分区1234ALTER TABLE logs ADD PARTITION (dt=&#x27;2023-10-01&#x27;, region=&#x27;Asia&#x27;);ALTER TABLE logs DROP PARTITION (dt=&#x27;2023-10-01&#x27;, region=&#x27;Asia&#x27;);ALTER TABLE logs PARTITION (dt=&#x27;2023-10-01&#x27;, region=&#x27;Asia&#x27;) SET LOCATION &#x27;/new/hdfs/path&#x27;; 修改表重命名表1ALTER TABLE employees RENAME TO staff; 添加列123ALTER TABLE staff ADD COLUMNS ( department STRING COMMENT &#x27;部门&#x27;); 修改列名&#x2F;类型12ALTER TABLE staff CHANGE COLUMN salary salary_info DOUBLE COMMENT &#x27;税前薪资&#x27;; 添加&#x2F;删除&#x2F;修改分区1234ALTER TABLE logs ADD PARTITION (dt=&#x27;2023-10-01&#x27;, region=&#x27;Asia&#x27;);ALTER TABLE logs DROP PARTITION (dt=&#x27;2023-10-01&#x27;, region=&#x27;Asia&#x27;);ALTER TABLE logs PARTITION (dt=&#x27;2023-10-01&#x27;, region=&#x27;Asia&#x27;) SET LOCATION &#x27;/new/hdfs/path&#x27;; 删除表12345DROP TABLE IF EXISTS staff;-- 外部表（只删除元数据）CREATE EXTERNAL TABLE ext_table (...);DROP TABLE ext_table; 清空表数据12TRUNCATE TABLE employees;TRUNCATE TABLE logs PARTITION (dt=&#x27;2023-10-01&#x27;); 使用 CTAS 创建表12CREATE TABLE IF NOT EXISTS new_tableAS SELECT ...; 内部表 vs 外部表 对比特性内部表 (Managed Table)外部表 (External Table)数据所有权/控制权Hive拥有并完全控制数据。​Hive只拥有元数据，数据由外部控制。​创建表时数据位置​可指定(LOCATION)，默认在仓库目录下创建。​​必须指定(LOCATION)，指向已有数据目录。​​​DROP TABLE操作​​删除元数据 AND 删除HDFS上的数据文件只删除元数据，HDFS上的数据文件保持不动。​数据生命周期​由Hive管理，表删数据失。独立于Hive存在，表删数据留。典型使用场景​Hive专用中间/临时表；最终结果表(需删数据)。共享数据源；外部分析；防止误删源数据；多应用共享。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://github.io/wobushick/Hexo/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"DDL","slug":"数据库/DDL","permalink":"https://github.io/wobushick/Hexo/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/DDL/"},{"name":"HQL","slug":"数据库/DDL/HQL","permalink":"https://github.io/wobushick/Hexo/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/DDL/HQL/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://github.io/wobushick/Hexo/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Hive","slug":"Hive","permalink":"https://github.io/wobushick/Hexo/tags/Hive/"}]},{"title":"Hive安装","slug":"Hive安装","date":"2025-07-12T16:00:00.000Z","updated":"2025-07-15T07:40:10.712Z","comments":true,"path":"2025/07/12/Hive安装/","permalink":"https://github.io/wobushick/Hexo/2025/07/12/Hive%E5%AE%89%E8%A3%85/","excerpt":"","text":"一、准备工作 ​​注意:​​ 以下操作默认您已配置好Hadoop环境 1. 下载Hive 下载地址：Hive 3.1.3 上传至Linux的/opt/software目录 2. 解压安装12tar -zxvf /opt/software/apache-hive-3.1.3-bin.tar.gz -C /opt/modulemv /opt/module/apache-hive-3.1.3-bin /opt/module/hive 二、环境配置1. 配置环境变量编辑/etc/profile文件：sudo vim /etc/profile 在文件底部添加： 123# HIVE_HOMEexport HIVE_HOME=/opt/module/hiveexport PATH=$PATH:$HIVE_HOME/bin 刷新配置文件：source /etc/profile 2. 同步Guava版本12rm $HIVE_HOME/lib/guava-*.jarcp /opt/hadoop-3.2.2/share/hadoop/common/lib/guava-*.jar $HIVE_HOME/lib/ 三、MySQL数据库配置1. 安装MySQL123sudo apt install -y mysql-server-8.0sudo systemctl start mysqlsudo systemctl enable mysql 2. 连接远程MySQLmysql -uroot -h192.168.1.3 -p123456 3. 安装MySQL JDBC驱动123wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.28.tar.gztar -xzvf mysql-connector-java-8.0.28.tar.gzcp mysql-connector-java-8.0.28/mysql-connector-java-8.0.28.jar $HIVE_HOME/lib/ 四、Hive配置1. 创建hive-site.xml12cd /opt/module/hive/confvim hive-site.xml 2. 添加配置内容12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;configuration&gt; &lt;!-- 指定Hive Metastore服务的URI（统一资源标识符） --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://192.168.1.6:9083&lt;/value&gt; &lt;!-- Metastore服务地址和端口 --&gt; &lt;/property&gt; &lt;!-- 指定是否使用本地Metastore服务（false表示使用远程服务） --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;!-- 设置为false启用远程Metastore --&gt; &lt;/property&gt; &lt;!-- 定义JDBC连接URL用于Metastore数据库 --&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://192.168.1.3:3306/hive_metastore?createDatabaseIfNotExist=true&amp;amp;useSSL=false&amp;amp;characterEncoding=UTF-8&lt;/value&gt; &lt;!-- jdbc:mysql://[数据库地址]:[端口]/[数据库名] createDatabaseIfNotExist=true: 自动创建数据库（首次启动时） useSSL=false: 禁用SSL连接（生产环境建议启用） characterEncoding=UTF-8: 设置字符编码 --&gt; &lt;/property&gt; &lt;!-- 指定JDBC驱动类名 --&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.cj.jdbc.Driver&lt;/value&gt; &lt;!-- MySQL JDBC驱动类 --&gt; &lt;/property&gt; &lt;!-- 定义连接Metastore数据库的用户名 --&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;!-- 数据库用户名 --&gt; &lt;/property&gt; &lt;!-- 定义连接Metastore数据库的密码 --&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;!-- 数据库密码 --&gt; &lt;/property&gt; &lt;!-- 指定Hive数据仓库的HDFS存储路径 --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;!-- 表数据默认存储位置 --&gt; &lt;/property&gt; &lt;!-- 指定Hive临时文件的HDFS存储路径 --&gt; &lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/user/hive/tmp&lt;/value&gt; &lt;!-- 作业执行时的临时文件目录 --&gt; &lt;/property&gt;&lt;/configuration&gt; 五、初始化与启动1. 初始化元数据12cd /opt/module/hive/bin/schematool -dbType mysql -initSchema --verbose 2. 启动Metastore服务hive --service metastore &gt; /tmp/metastore.log 2&gt;&amp;1 &amp; 3. 验证端口监听netstat -tuln | grep 9083 六、连接模式示意图","categories":[{"name":"数据平台","slug":"数据平台","permalink":"https://github.io/wobushick/Hexo/categories/%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0/"},{"name":"Hive","slug":"数据平台/Hive","permalink":"https://github.io/wobushick/Hexo/categories/%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0/Hive/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://github.io/wobushick/Hexo/tags/Hive/"},{"name":"HQL","slug":"HQL","permalink":"https://github.io/wobushick/Hexo/tags/HQL/"}]}],"categories":[{"name":"数据库","slug":"数据库","permalink":"https://github.io/wobushick/Hexo/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"DML","slug":"数据库/DML","permalink":"https://github.io/wobushick/Hexo/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/DML/"},{"name":"HQL","slug":"数据库/DML/HQL","permalink":"https://github.io/wobushick/Hexo/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/DML/HQL/"},{"name":"DDL","slug":"数据库/DDL","permalink":"https://github.io/wobushick/Hexo/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/DDL/"},{"name":"HQL","slug":"数据库/DDL/HQL","permalink":"https://github.io/wobushick/Hexo/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/DDL/HQL/"},{"name":"数据平台","slug":"数据平台","permalink":"https://github.io/wobushick/Hexo/categories/%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0/"},{"name":"Hive","slug":"数据平台/Hive","permalink":"https://github.io/wobushick/Hexo/categories/%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0/Hive/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://github.io/wobushick/Hexo/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Hive","slug":"Hive","permalink":"https://github.io/wobushick/Hexo/tags/Hive/"},{"name":"HQL","slug":"HQL","permalink":"https://github.io/wobushick/Hexo/tags/HQL/"}]}